{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ИИ в Colab"
      ],
      "metadata": {
        "id": "yY8UciRitO48"
      }
    },
    {
      "metadata": {
        "id": "wdj9RMfoGPC2"
      },
      "cell_type": "markdown",
      "source": [
        "Colab упрощает интеграцию мощных возможностей генеративного ИИ в ваши проекты. Мы запускаем публичную предварительную версию простой и интуитивно понятной библиотеки Python (google.colab.ai), позволяющей получать доступ к передовым языковым моделям непосредственно в средах Colab. Все пользователи имеют бесплатный доступ к большинству популярных языковых моделей, а платные пользователи получают доступ к более широкому выбору моделей. Это означает, что пользователи могут тратить меньше времени на настройку и установку и больше времени на воплощение своих идей в жизнь. Всего несколькими строками кода вы теперь можете выполнять множество задач:\n",
        "- Генерировать текст\n",
        "- Переводить языки\n",
        "- Создавать креативный контент\n",
        "- Классифицировать текст\n",
        "\n",
        "Бесплатный Colab\n",
        "~50–100 запросов в день\n",
        "~100K–200K токенов/день\n",
        "\n",
        "**Не загружайте конфеденциальные данные!!!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Базовые операции"
      ],
      "metadata": {
        "id": "MzzzHV8HtWss"
      }
    },
    {
      "metadata": {
        "id": "Ucchuu5vV3Jp"
      },
      "cell_type": "code",
      "source": [
        "# @title List available models\n",
        "from google.colab import ai\n",
        "\n",
        "ai.list_models()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "LjfCGEpzDsD9"
      },
      "cell_type": "markdown",
      "source": [
        "Выбор модели. Названия моделей дают представление об их возможностях и предполагаемом использовании:\n",
        "\n",
        "Pro: Это наиболее функциональные модели, идеально подходящие для сложных логических рассуждений, творческих задач и детального анализа.\n",
        "\n",
        "Flash: Эти модели оптимизированы для высокой скорости и эффективности, что делает их отличными для суммаризации, чат-приложений и задач, требующих быстрой реакции.\n",
        "\n",
        "Gemma: Это легкие, открытые модели, подходящие для различных задач генерации текста и отлично подходящие для экспериментов."
      ]
    },
    {
      "metadata": {
        "id": "R7taibpc7x2l"
      },
      "cell_type": "code",
      "source": [
        "# @title Simple batch generation example\n",
        "# Only text-to-text input/output is supported\n",
        "from google.colab import ai\n",
        "\n",
        "response = ai.generate_text(\"В каком году была принята конституция РФ?\")\n",
        "print(response)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Choose a different model\n",
        "from google.colab import ai\n",
        "\n",
        "response = ai.generate_text(\"What is the capital of England\", model_name='google/gemini-2.5-flash-lite')\n",
        "print(response)"
      ],
      "metadata": {
        "id": "DDZ9786umFRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ysDdFbH_Dgtz"
      },
      "cell_type": "markdown",
      "source": [
        "Для генерации длинных текстовых файлов можно использовать потоковую передачу ответа. В этом случае выходные токены отображаются по мере их генерации, а не дожидаются завершения генерации всего ответа. Это обеспечивает более интерактивный и быстрый отклик. Чтобы включить эту функцию, просто установите параметр stream=True."
      ]
    },
    {
      "metadata": {
        "id": "4BNgxiB6--_5"
      },
      "cell_type": "code",
      "source": [
        "# @title Simple streaming example\n",
        "from google.colab import ai\n",
        "\n",
        "stream = ai.generate_text(\"Подготовь мне лекцию о природе Дальнего Востока\", stream=True)\n",
        "for text in stream:\n",
        "  print(text, end='')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Особое форматирование"
      ],
      "metadata": {
        "id": "GMuauVOOtldn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LineWrapper — умное форматирование текста с автоматическим переносом строк по заданной максимальной ширине (по умолчанию 80 символов),"
      ],
      "metadata": {
        "id": "Tv6jCyxgn2SS"
      }
    },
    {
      "metadata": {
        "id": "CpMmpaVClSBV",
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title Text formatting setup\n",
        "#code is not necessary for colab.ai, but is useful in fomatting text chunks\n",
        "import sys\n",
        "\n",
        "class LineWrapper:\n",
        "    def __init__(self, max_length=80):\n",
        "        self.max_length = max_length\n",
        "        self.current_line_length = 0\n",
        "\n",
        "    def print(self, text_chunk):\n",
        "        i = 0\n",
        "        n = len(text_chunk)\n",
        "        while i < n:\n",
        "            start_index = i\n",
        "            while i < n and text_chunk[i] not in ' \\n': # Find end of word\n",
        "                i += 1\n",
        "            current_word = text_chunk[start_index:i]\n",
        "\n",
        "            delimiter = \"\"\n",
        "            if i < n: # If not end of chunk, we found a delimiter\n",
        "                delimiter = text_chunk[i]\n",
        "                i += 1 # Consume delimiter\n",
        "\n",
        "            if current_word:\n",
        "                needs_leading_space = (self.current_line_length > 0)\n",
        "\n",
        "                # Case 1: Word itself is too long for a line (must be broken)\n",
        "                if len(current_word) > self.max_length:\n",
        "                    if needs_leading_space: # Newline if current line has content\n",
        "                        sys.stdout.write('\\n')\n",
        "                        self.current_line_length = 0\n",
        "                    for char_val in current_word: # Break the long word\n",
        "                        if self.current_line_length >= self.max_length:\n",
        "                            sys.stdout.write('\\n')\n",
        "                            self.current_line_length = 0\n",
        "                        sys.stdout.write(char_val)\n",
        "                        self.current_line_length += 1\n",
        "                # Case 2: Word doesn't fit on current line (print on new line)\n",
        "                elif self.current_line_length + (1 if needs_leading_space else 0) + len(current_word) > self.max_length:\n",
        "                    sys.stdout.write('\\n')\n",
        "                    sys.stdout.write(current_word)\n",
        "                    self.current_line_length = len(current_word)\n",
        "                # Case 3: Word fits on current line\n",
        "                else:\n",
        "                    if needs_leading_space:\n",
        "                        # Define punctuation that should not have a leading space\n",
        "                        # when they form an entire \"word\" (token) following another word.\n",
        "                        no_leading_space_punctuation = {\n",
        "                            \",\", \".\", \";\", \":\", \"!\", \"?\",        # Standard sentence punctuation\n",
        "                            \")\", \"]\", \"}\",                     # Closing brackets\n",
        "                            \"'s\", \"'S\", \"'re\", \"'RE\", \"'ve\", \"'VE\", # Common contractions\n",
        "                            \"'m\", \"'M\", \"'ll\", \"'LL\", \"'d\", \"'D\",\n",
        "                            \"n't\", \"N'T\",\n",
        "                            \"...\", \"…\"                          # Ellipses\n",
        "                        }\n",
        "                        if current_word not in no_leading_space_punctuation:\n",
        "                            sys.stdout.write(' ')\n",
        "                            self.current_line_length += 1\n",
        "                    sys.stdout.write(current_word)\n",
        "                    self.current_line_length += len(current_word)\n",
        "\n",
        "            if delimiter == '\\n':\n",
        "                sys.stdout.write('\\n')\n",
        "                self.current_line_length = 0\n",
        "            elif delimiter == ' ':\n",
        "                # If line is full and a space delimiter arrives, it implies a wrap.\n",
        "                if self.current_line_length >= self.max_length:\n",
        "                    sys.stdout.write('\\n')\n",
        "                    self.current_line_length = 0\n",
        "\n",
        "        sys.stdout.flush()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "DWiLPzTnRoy-"
      },
      "cell_type": "code",
      "source": [
        "# @title Formatted streaming example\n",
        "from google.colab import ai\n",
        "\n",
        "wrapper = LineWrapper()\n",
        "for chunk in ai.generate_text('Подготовь доклад о римской империи.', model_name='google/gemini-2.5-flash-lite', stream=True):\n",
        "  wrapper.print(chunk)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Организация диалога"
      ],
      "metadata": {
        "id": "6QmhkZ-rtq2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = input(\"Введите ваш вопрос: \")\n",
        "print(f\"Вы спросили: {user_question}\")\n",
        "\n",
        "# Пример использования с генерацией текста\n",
        "from google.colab import ai\n",
        "response = ai.generate_text(user_question)\n",
        "print(\"\\nОтвет:\", response)"
      ],
      "metadata": {
        "id": "2lGcG8IeoeYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import ai\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "output = widgets.Output()\n",
        "text_input = widgets.Text(\n",
        "    description='Вопрос:',\n",
        "    placeholder='Введите вопрос здесь...',\n",
        "    layout=widgets.Layout(width='80%')\n",
        ")\n",
        "\n",
        "def on_submit(sender):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        print(f\" Вопрос: {text_input.value}\")\n",
        "        print(\"Генерация ответа...\")\n",
        "        response = ai.generate_text(text_input.value)\n",
        "        clear_output()\n",
        "        print(f\"Вопрос: {text_input.value}\")\n",
        "        print(f\"Ответ: {response}\\n\")\n",
        "\n",
        "text_input.on_submit(on_submit)\n",
        "display(text_input, output)"
      ],
      "metadata": {
        "id": "GwyR2LQsorZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы организовать диалог в Google Colab с «памятью» (сохранением контекста предыдущих реплик), важно понимать: ai.generate_text() сам по себе НЕ запоминает историю — каждый вызов независим. Чтобы модель «помнила» разговор, нужно вручную передавать историю в промпт."
      ],
      "metadata": {
        "id": "HDFrAp5Gprxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import ai\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "history = []\n",
        "chat_output = widgets.Output(layout={'border': '1px solid #ddd', 'height': '300px', 'overflow': 'auto'})\n",
        "input_box = widgets.Text(description='Вы:', placeholder='Введите сообщение...', layout=widgets.Layout(width='80%'))\n",
        "\n",
        "def on_submit(sender):\n",
        "    question = input_box.value.strip()\n",
        "    if not question:\n",
        "        return\n",
        "\n",
        "    # Формируем промпт с контекстом\n",
        "    prompt = \"Ты — дружелюбный ассистент. Отвечай естественно.\\n\\n\"\n",
        "    for role, text in history[-6:]:  # Берём последние 3 пары реплик (ограничение длины)\n",
        "        prompt += f\"{role}: {text}\\n\"\n",
        "    prompt += f\"Пользователь: {question}\\nАссистент:\"\n",
        "\n",
        "    # Генерация\n",
        "    with chat_output:\n",
        "        print(f\"Вы: {question}\")\n",
        "        print(\"ИИ думает...\", end=\"\\r\")\n",
        "        response = ai.generate_text(prompt)\n",
        "        print(\" \" * 30, end=\"\\r\")  # Стираем \"думает\"\n",
        "        print(f\"ИИ: {response}\\n\")\n",
        "\n",
        "    # Сохраняем в историю\n",
        "    history.append((\"Пользователь\", question))\n",
        "    history.append((\"Ассистент\", response))\n",
        "    input_box.value = \"\"\n",
        "\n",
        "input_box.on_submit(on_submit)\n",
        "display(input_box, chat_output)"
      ],
      "metadata": {
        "id": "4lXRpcgFpw9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обработка данных"
      ],
      "metadata": {
        "id": "aJF5i-jEtxG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import ai\n",
        "\n",
        "table = \"\"\"\n",
        "Город,Население,Страна\n",
        "Москва,12500000,Россия\n",
        "Париж,2160000,Франция\n",
        "Токио,13900000,Япония\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"Проанализируй таблицу и ответь на вопрос.\n",
        "\n",
        "Таблица:\n",
        "{table}\n",
        "\n",
        "Вопрос: Какой город самый густонаселённый?\"\"\"\n",
        "\n",
        "response = ai.generate_text(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "IYBHoP3FqlwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files, ai\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Загружаем файл\n",
        "uploaded = files.upload()  # Откроется окно выбора файла\n",
        "\n",
        "# 2. Читаем таблицу\n",
        "df = pd.read_csv(list(uploaded.keys())[0])\n",
        "\n",
        "# 3. Преобразуем в текст (лучше — в формате Markdown для читаемости)\n",
        "table_text = df.to_markdown(index=False)\n",
        "\n",
        "# 4. Запрашиваем анализ\n",
        "prompt = f\"\"\"Таблица с данными:\n",
        "\n",
        "{table_text}\n",
        "\n",
        "Вопрос: Сколько всего строк в таблице и какие столбцы присутствуют?\n",
        "\n",
        "Расчитатай все возможные статсические характеристики.\n",
        "\"\"\"\n",
        "\n",
        "response = ai.generate_text(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "_lKo_wY2rD6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import ai\n",
        "\n",
        "# 1. Загружаем и анализируем данные программно\n",
        "df = pd.read_csv('data.csv')\n",
        "stats = df.describe().to_markdown()\n",
        "\n",
        "# 2. Передаём агрегированные данные + вопрос модели\n",
        "prompt = f\"\"\"Статистика по таблице:\n",
        "\n",
        "{stats}\n",
        "\n",
        "Вопрос: Какие аномалии ты заметил в распределении данных? Дай рекомендации.\"\"\"\n",
        "\n",
        "response = ai.generate_text(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "xmW2vvP7shhI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}